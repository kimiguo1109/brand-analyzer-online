#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å“ç‰Œåˆ†æå™¨ APIåç«¯
"""

import os
import sys
import json
import time
import logging
import threading
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed
from flask import Flask, request, send_file, jsonify
from flask_cors import CORS
from werkzeug.utils import secure_filename
import uuid

# å¯¼å…¥æˆ‘ä»¬çš„åˆ†æå™¨
from universal_brand_analyzer import UniversalBrandAnalyzer
# from convert_csv_to_json_v2 import TikTokDataConverter  # ä¸å†éœ€è¦

app = Flask(__name__)
CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

# é…ç½®
UPLOAD_FOLDER = 'uploads'
RESULTS_FOLDER = 'analyzed_data'
ALLOWED_EXTENSIONS = {'json', 'csv'}

app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER
app.config['MAX_CONTENT_LENGTH'] = 50 * 1024 * 1024  # 50MB max file size

# ç¡®ä¿æ–‡ä»¶å¤¹å­˜åœ¨
os.makedirs(UPLOAD_FOLDER, exist_ok=True)
os.makedirs(RESULTS_FOLDER, exist_ok=True)

# å­˜å‚¨åˆ†æä»»åŠ¡çš„å…¨å±€å­—å…¸
analysis_tasks = {}

# ä»»åŠ¡æ¸…ç†é…ç½®
MAX_TASKS_IN_MEMORY = 50  # æœ€å¤§ä¿å­˜çš„ä»»åŠ¡æ•°é‡
TASK_TIMEOUT_HOURS = 24   # ä»»åŠ¡è¶…æ—¶æ—¶é—´ï¼ˆå°æ—¶ï¼‰

# çº¿ç¨‹æ± é…ç½® - 10ä¸ªå·¥ä½œçº¿ç¨‹
MAX_WORKERS = 10
thread_pool = ThreadPoolExecutor(max_workers=MAX_WORKERS)
print(f"ğŸš€ Initialized thread pool with {MAX_WORKERS} workers for high-performance analysis")

# åº”ç”¨å…³é—­æ—¶æ¸…ç†çº¿ç¨‹æ± 
import atexit

def cleanup_thread_pool():
    """åº”ç”¨å…³é—­æ—¶æ¸…ç†çº¿ç¨‹æ± """
    print("ğŸ”„ Shutting down thread pool...")
    thread_pool.shutdown(wait=True)
    print("âœ… Thread pool shutdown complete")

atexit.register(cleanup_thread_pool)

def cleanup_old_tasks():
    """æ¸…ç†æ—§ä»»åŠ¡ä»¥é˜²æ­¢å†…å­˜æ³„æ¼"""
    current_time = datetime.now()
    tasks_to_remove = []
    
    for task_id, task_data in analysis_tasks.items():
        try:
            # æ£€æŸ¥ä»»åŠ¡åˆ›å»ºæ—¶é—´
            created_at_str = task_data.get('created_at')
            if created_at_str:
                created_at = datetime.fromisoformat(created_at_str.replace('Z', '+00:00').replace('+00:00', ''))
                age_hours = (current_time - created_at).total_seconds() / 3600
                
                # æ¸…ç†è¶…è¿‡24å°æ—¶çš„ä»»åŠ¡
                if age_hours > TASK_TIMEOUT_HOURS:
                    tasks_to_remove.append(task_id)
                    print(f"Marking task {task_id} for cleanup (age: {age_hours:.1f} hours)")
            
        except Exception as e:
            print(f"Error checking task {task_id} age: {e}")
            # å¦‚æœæ— æ³•è§£ææ—¶é—´ï¼Œä¹Ÿæ ‡è®°ä¸ºåˆ é™¤
            tasks_to_remove.append(task_id)
    
    # å¦‚æœä»»åŠ¡æ•°é‡è¶…è¿‡é™åˆ¶ï¼Œæ¸…ç†æœ€æ—§çš„å·²å®Œæˆä»»åŠ¡
    if len(analysis_tasks) > MAX_TASKS_IN_MEMORY:
        completed_tasks = [(task_id, task_data) for task_id, task_data in analysis_tasks.items() 
                          if task_data.get('status') in ['completed', 'error']]
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åºï¼Œåˆ é™¤æœ€æ—§çš„
        completed_tasks.sort(key=lambda x: x[1].get('created_at', ''))
        excess_count = len(analysis_tasks) - MAX_TASKS_IN_MEMORY
        
        for i in range(min(excess_count, len(completed_tasks))):
            task_id = completed_tasks[i][0]
            if task_id not in tasks_to_remove:
                tasks_to_remove.append(task_id)
                print(f"Marking task {task_id} for cleanup (memory limit)")
    
    # æ‰§è¡Œæ¸…ç†
    for task_id in tasks_to_remove:
        try:
            del analysis_tasks[task_id]
            print(f"Cleaned up task {task_id}")
        except KeyError:
            pass
    
    if tasks_to_remove:
        print(f"Cleaned up {len(tasks_to_remove)} old tasks. Remaining: {len(analysis_tasks)}")

def start_cleanup_thread():
    """å¯åŠ¨æ¸…ç†çº¿ç¨‹"""
    def cleanup_worker():
        while True:
            try:
                cleanup_old_tasks()
                time.sleep(3600)  # æ¯å°æ—¶æ¸…ç†ä¸€æ¬¡
            except Exception as e:
                print(f"Error in cleanup thread: {e}")
                time.sleep(600)  # å‡ºé”™æ—¶ç­‰å¾…10åˆ†é’Ÿå†è¯•
    
    cleanup_thread = threading.Thread(target=cleanup_worker, daemon=True)
    cleanup_thread.start()
    print("Started task cleanup thread")

def allowed_file(filename):
    return '.' in filename and \
           filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS

class WebAppHandler(logging.Handler):
    """è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨ï¼Œç”¨äºå®æ—¶æ˜¾ç¤ºæ—¥å¿—"""
    
    def __init__(self, task_id):
        super().__init__()
        self.task_id = task_id
        self.logs = []
    
    def emit(self, record):
        log_entry = self.format(record)
        self.logs.append({
            'timestamp': datetime.now().strftime('%H:%M:%S'),
            'level': record.levelname,
            'message': log_entry
        })
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        if self.task_id in analysis_tasks:
            analysis_tasks[self.task_id]['logs'] = self.logs

# LoggingCSVConverterç±»å·²ç§»é™¤ï¼Œç°åœ¨ä½¿ç”¨ç›´æ¥CSVåˆ†ææ–¹æ³•

def run_analysis(task_id, file_path, is_csv=False):
    """åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œåˆ†æä»»åŠ¡ - æ”¯æŒé«˜å¹¶å‘å¤„ç†"""
    try:
        # ç¡®ä¿ä»»åŠ¡å­˜åœ¨äºå­—å…¸ä¸­
        if task_id not in analysis_tasks:
            print(f"Warning: Task {task_id} not found in analysis_tasks")
            return
            
        print(f"ğŸš€ [Worker Thread] Starting analysis for task {task_id} (Pool capacity: {MAX_WORKERS})")
        start_time = time.time()
            
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        analysis_tasks[task_id]['status'] = 'running'
        analysis_tasks[task_id]['progress'] = 'Starting analysis...'
        
        print(f"Starting analysis for task {task_id}, file: {file_path}, is_csv: {is_csv}")
        
        # åˆ›å»ºè‡ªå®šä¹‰loggerï¼ˆæ— è®ºæ˜¯å¦CSVéƒ½éœ€è¦ï¼‰
        custom_logger = logging.getLogger(f'analyzer_{task_id}')
        custom_logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        custom_logger.handlers = []
        
        # è®¾ç½®è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨
        handler = WebAppHandler(task_id)
        handler.setFormatter(logging.Formatter('%(message)s'))
        custom_logger.addHandler(handler)
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹ï¼Œä¼ å…¥è‡ªå®šä¹‰logger
        print(f"Creating analyzer for task {task_id}")
        analyzer = UniversalBrandAnalyzer(output_dir=RESULTS_FOLDER, custom_logger=custom_logger)
        
        # å¦‚æœæ˜¯CSVæ–‡ä»¶ï¼Œä½¿ç”¨æ–°çš„ç›´æ¥åˆ†ææ–¹æ³•
        if is_csv:
            custom_logger.info('ğŸ“‚ CSV file detected, using direct analysis...')
            analysis_tasks[task_id]['progress'] = 'Starting direct CSV analysis...'
            
            # æ£€æŸ¥CSVæ˜¯å¦å·²ç»åŒ…å«åˆ†æç»“æœ
            custom_logger.info('ğŸ” Checking if CSV already contains analysis results...')
            results = check_and_process_analyzed_csv(file_path, custom_logger)
            
            if not results:
                # å¦‚æœæ²¡æœ‰ç°æˆçš„åˆ†æç»“æœï¼Œåˆ™è¿›è¡Œæ–°çš„åˆ†æ
                print(f"Running direct CSV analysis for task {task_id}")
                analysis_tasks[task_id]['progress'] = 'Processing CSV data...'
                custom_logger.info('ğŸ“Š CSV needs analysis, starting direct CSV analysis...')
                results = analyzer.analyze_creators_from_csv_direct(file_path)
        else:
            custom_logger.info('ğŸ“„ JSON file detected, starting analysis...')
            analysis_tasks[task_id]['progress'] = 'JSONæ–‡ä»¶æ£€æµ‹å®Œæˆï¼Œå¼€å§‹åˆ†æ...'
            
            # è¿è¡Œåˆ†æ
            print(f"Running JSON analysis for task {task_id}")
            analysis_tasks[task_id]['progress'] = 'æ­£åœ¨è¿›è¡Œå“ç‰Œåˆ†æ...'
            results = analyzer.analyze_creators(file_path)
        
        print(f"Analysis completed for task {task_id}, results count: {len(results) if results else 0}")
        
        if results:
            # ä¿å­˜ç»“æœ
            custom_logger.info('ğŸ’¾ Saving analysis results...')
            analysis_tasks[task_id]['progress'] = 'ä¿å­˜åˆ†æç»“æœ...'
            analyzer.save_results(results, file_path)
            
            # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            
            # ä¸ä½¿ç”¨æ—¶é—´æˆ³ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡ä»¶åï¼ˆä¸run_csv_analysisä¿æŒä¸€è‡´ï¼‰
            brand_file = f"{base_name}_brand_related.csv"
            non_brand_file = f"{base_name}_non_brand.csv"
            
            # ç”Ÿæˆåˆå¹¶åçš„ç»“æœæ–‡ä»¶
            merged_file = f"{base_name}_merged_results.csv"
            
            # å¦‚æœæ˜¯CSVæ–‡ä»¶ï¼Œæ‰§è¡Œåˆå¹¶é€»è¾‘
            if is_csv:
                # å¦‚æœCSVå·²ç»åŒ…å«åˆ†æç»“æœï¼Œç›´æ¥å¤åˆ¶ä¸ºmergedæ–‡ä»¶
                if os.path.exists(file_path) and file_path.endswith('.csv'):
                    # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯åˆ†æè¿‡çš„CSV
                    try:
                        import pandas as pd
                        df = pd.read_csv(file_path)
                        analysis_fields = ['account_type', 'brand', 'email']
                        has_analysis_fields = all(field in df.columns for field in analysis_fields)
                        
                        # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                        has_valid_data = False
                        if has_analysis_fields:
                            non_empty_account_types = df['account_type'].notna() & (df['account_type'] != '')
                            non_empty_brands = df['brand'].notna() & (df['brand'] != '')
                            valid_rows = non_empty_account_types.sum() + non_empty_brands.sum()
                            has_valid_data = valid_rows > len(df) * 0.1
                        
                        if has_analysis_fields and has_valid_data:
                            # ç›´æ¥å¤åˆ¶ä¸ºmergedæ–‡ä»¶
                            import shutil
                            merged_path = os.path.join(RESULTS_FOLDER, merged_file)
                            shutil.copy2(file_path, merged_path)
                            custom_logger.info('âœ… CSV already contains analysis results, copied as merged file')
                            merge_success = True
                        else:
                            # æ‰§è¡Œåˆå¹¶é€»è¾‘
                            custom_logger.info('ğŸ”— Merging results with original data...')
                            analysis_tasks[task_id]['progress'] = 'Merging results...'
                            merge_success = merge_results_with_original(file_path, os.path.join(RESULTS_FOLDER, brand_file), merged_file, custom_logger)
                    except Exception as e:
                        custom_logger.error(f'âŒ Error checking CSV format: {e}')
                        merge_success = False
                else:
                    merge_success = False
            else:
                # å¯¹äºJSONæ–‡ä»¶ï¼Œä¸è¿›è¡Œåˆå¹¶
                merge_success = False
            
            # é¦–å…ˆå®šä¹‰å“ç‰Œç›¸å…³è´¦å·ï¼šæœ‰extracted_brand_nameæˆ–è€…è¢«æ ‡è®°ä¸ºå“ç‰Œ/çŸ©é˜µè´¦å·
            brand_related_results = [r for r in results if 
                                   (r.extracted_brand_name and r.extracted_brand_name.strip()) or 
                                   r.is_brand or r.is_matrix_account]
            non_brand_results = [r for r in results if r not in brand_related_results]
            
            # æŒ‰ç…§account_typeç»Ÿè®¡æ€»åˆ›ä½œè€…ä¸­çš„å„ç±»å‹æ•°é‡ï¼ˆäº’æ–¥åˆ†ç±»ï¼‰
            total_creators = len(results)
            official_account_count = sum(1 for r in results if r.is_brand)
            matrix_account_count = sum(1 for r in results if r.is_matrix_account)
            # UGCåˆ›ä½œè€…ï¼šåœ¨å“ç‰Œç›¸å…³ä¸­ä¸”æ ‡è®°ä¸ºUGCçš„åˆ›ä½œè€…
            ugc_creator_count = sum(1 for r in brand_related_results if r.is_ugc_creator)
            # éå“ç‰Œåˆ›ä½œè€…ï¼šä¸åœ¨å“ç‰Œç›¸å…³åˆ—è¡¨ä¸­çš„æ‰€æœ‰åˆ›ä½œè€…
            non_branded_creator_count = len(non_brand_results)
            
            # å“ç‰Œç›¸å…³è´¦å·ä¸­çš„åˆ†ç±»ç»Ÿè®¡ï¼ˆç”¨äºBrand Related Breakdownï¼‰
            brand_in_related = sum(1 for r in brand_related_results if r.is_brand)
            matrix_in_related = sum(1 for r in brand_related_results if r.is_matrix_account)
            ugc_in_related = sum(1 for r in brand_related_results if r.is_ugc_creator)
            
            # ç¡®ä¿ä»»åŠ¡ä»ç„¶å­˜åœ¨äºå­—å…¸ä¸­ï¼ˆé˜²æ­¢è¢«æ¸…ç†ï¼‰
            if task_id in analysis_tasks:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€
                analysis_tasks[task_id].update({
                    'status': 'completed',
                    'progress': 'ğŸ‰ åˆ†æå®Œæˆï¼',
                    'completed_at': datetime.now().isoformat(),
                    'results': {
                        'total_processed': total_creators,
                        'brand_related_count': len(brand_related_results),
                        'non_brand_count': total_creators - len(brand_related_results),
                        'official_account_count': official_account_count,
                        'matrix_account_count': matrix_account_count,
                        'ugc_creator_count': ugc_creator_count,
                        'non_branded_creator_count': non_branded_creator_count,
                        # å„ç±»å‹åœ¨æ€»åˆ›ä½œè€…ä¸­çš„ç™¾åˆ†æ¯”
                        'official_account_percentage': round((official_account_count / total_creators) * 100) if total_creators > 0 else 0,
                        'matrix_account_percentage': round((matrix_account_count / total_creators) * 100) if total_creators > 0 else 0,
                        'ugc_creator_percentage': round((ugc_creator_count / total_creators) * 100) if total_creators > 0 else 0,
                        'non_branded_creator_percentage': round((non_branded_creator_count / total_creators) * 100) if total_creators > 0 else 0,
                        # Brand Related Breakdown - åœ¨å“ç‰Œç›¸å…³è´¦å·ä¸­çš„ç™¾åˆ†æ¯”
                        'brand_in_related': brand_in_related,
                        'matrix_in_related': matrix_in_related,
                        'ugc_in_related': ugc_in_related,
                        'brand_in_related_percentage': round((brand_in_related / len(brand_related_results)) * 100) if len(brand_related_results) > 0 else 0,
                        'matrix_in_related_percentage': round((matrix_in_related / len(brand_related_results)) * 100) if len(brand_related_results) > 0 else 0,
                        'ugc_in_related_percentage': round((ugc_in_related / len(brand_related_results)) * 100) if len(brand_related_results) > 0 else 0,
                        'brand_file': brand_file,
                        'non_brand_file': non_brand_file,
                        'merged_file': merged_file if 'merge_success' in locals() and merge_success else None
                    }
                })
                custom_logger.info(f'âœ… Analysis completed successfully! Processed {len(results)} creators')
                print(f"Task {task_id} completed successfully")
            else:
                print(f"Warning: Task {task_id} was removed from analysis_tasks during processing")
        else:
            if task_id in analysis_tasks:
                analysis_tasks[task_id]['status'] = 'error'
                analysis_tasks[task_id]['progress'] = 'åˆ†æå¤±è´¥ï¼šæ²¡æœ‰æˆåŠŸåˆ†æä»»ä½•åˆ›ä½œè€…'
            custom_logger.error('âŒ Analysis failed: No creators were successfully analyzed')
            print(f"Analysis failed for task {task_id}: No results")
            
    except Exception as e:
        error_msg = f'åˆ†æå¤±è´¥ï¼š{str(e)}'
        print(f"Exception in run_analysis for task {task_id}: {str(e)}")
        
        # ç¡®ä¿ä»»åŠ¡ä»ç„¶å­˜åœ¨
        if task_id in analysis_tasks:
            analysis_tasks[task_id]['status'] = 'error'
            analysis_tasks[task_id]['progress'] = error_msg
            analysis_tasks[task_id]['error_at'] = datetime.now().isoformat()
        
        # ä¹Ÿå°è¯•è®°å½•åˆ°æ—¥å¿—
        try:
            if 'custom_logger' in locals():
                custom_logger.error(f'âŒ Analysis error: {str(e)}')
        except:
            pass
    
    finally:
        # æ€§èƒ½ç»Ÿè®¡
        if 'start_time' in locals():
            execution_time = time.time() - start_time
            print(f"â±ï¸ [Worker Thread] Task {task_id} execution time: {execution_time:.2f}s")
            
            # æ·»åŠ æ€§èƒ½ä¿¡æ¯åˆ°ä»»åŠ¡æ•°æ®ä¸­
            if task_id in analysis_tasks:
                analysis_tasks[task_id]['execution_time'] = execution_time
                analysis_tasks[task_id]['finished_at'] = datetime.now().isoformat()
        
        # æ¸…ç†è‡ªå®šä¹‰logger
        try:
            if 'custom_logger' in locals():
                custom_logger.handlers = []
        except:
            pass
        
        print(f"ğŸ [Worker Thread] Analysis thread for task {task_id} finished")

def run_csv_analysis(task_id, file_path):
    """åœ¨çº¿ç¨‹æ± ä¸­ç›´æ¥åˆ†æCSVæ–‡ä»¶ - é«˜æ€§èƒ½å¹¶å‘å¤„ç†"""
    try:
        # ç¡®ä¿ä»»åŠ¡å­˜åœ¨äºå­—å…¸ä¸­
        if task_id not in analysis_tasks:
            print(f"Warning: Task {task_id} not found in analysis_tasks")
            return
            
        print(f"ğŸš€ [CSV Worker] Starting CSV analysis for task {task_id} (Pool: {MAX_WORKERS} workers)")
        start_time = time.time()
            
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€
        analysis_tasks[task_id]['status'] = 'running'
        analysis_tasks[task_id]['progress'] = 'Starting CSV analysis...'
        
        print(f"Starting CSV analysis for task {task_id}, file: {file_path}")
        
        # åˆ›å»ºè‡ªå®šä¹‰logger
        custom_logger = logging.getLogger(f'csv_analyzer_{task_id}')
        custom_logger.setLevel(logging.INFO)
        
        # æ¸…é™¤ç°æœ‰å¤„ç†å™¨
        custom_logger.handlers = []
        
        # è®¾ç½®è‡ªå®šä¹‰æ—¥å¿—å¤„ç†å™¨
        handler = WebAppHandler(task_id)
        handler.setFormatter(logging.Formatter('%(message)s'))
        custom_logger.addHandler(handler)
        
        custom_logger.info('ğŸ“„ Starting CSV analysis...')
        analysis_tasks[task_id]['progress'] = 'CSV analysis started...'
        
        # åˆ›å»ºåˆ†æå™¨å®ä¾‹ï¼Œä¼ å…¥è‡ªå®šä¹‰logger
        print(f"Creating analyzer for task {task_id}")
        analyzer = UniversalBrandAnalyzer(output_dir=RESULTS_FOLDER, custom_logger=custom_logger)
        
        # æ£€æŸ¥CSVæ˜¯å¦å·²ç»åŒ…å«åˆ†æç»“æœ
        custom_logger.info('ğŸ” Checking if CSV already contains analysis results...')
        results = check_and_process_analyzed_csv(file_path, custom_logger)
        
        if not results:
            # å¦‚æœæ²¡æœ‰ç°æˆçš„åˆ†æç»“æœï¼Œåˆ™è¿›è¡Œæ–°çš„åˆ†æ
            print(f"Running CSV analysis for task {task_id}")
            analysis_tasks[task_id]['progress'] = 'Processing CSV data...'
            custom_logger.info('ğŸ“Š CSV needs analysis, starting direct CSV analysis...')
            results = analyzer.analyze_creators_from_csv_direct(file_path)
        
        print(f"CSV analysis completed for task {task_id}, results count: {len(results) if results else 0}")
        
        if results:
            # ä¿å­˜ç»“æœ
            custom_logger.info('ğŸ’¾ Saving analysis results...')
            analysis_tasks[task_id]['progress'] = 'Saving analysis results...'
            analyzer.save_results(results, file_path)
            
            # ç”Ÿæˆç»“æœæ–‡ä»¶è·¯å¾„
            base_name = os.path.splitext(os.path.basename(file_path))[0]
            
            # ä¸ä½¿ç”¨æ—¶é—´æˆ³ï¼Œä½¿ç”¨å›ºå®šçš„æ–‡ä»¶å
            brand_file = f"{base_name}_brand_related.csv"
            non_brand_file = f"{base_name}_non_brand.csv"
            
            # ç”Ÿæˆåˆå¹¶åçš„ç»“æœæ–‡ä»¶
            merged_file = f"{base_name}_merged_results.csv"
            
            # å¦‚æœCSVå·²ç»åŒ…å«åˆ†æç»“æœï¼Œç›´æ¥å¤åˆ¶ä¸ºmergedæ–‡ä»¶
            if os.path.exists(file_path) and file_path.endswith('.csv'):
                # æ£€æŸ¥æ˜¯å¦å·²ç»æ˜¯åˆ†æè¿‡çš„CSV
                try:
                    import pandas as pd
                    df = pd.read_csv(file_path)
                    analysis_fields = ['account_type', 'brand', 'email']
                    has_analysis_fields = all(field in df.columns for field in analysis_fields)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆæ•°æ®
                    has_valid_data = False
                    if has_analysis_fields:
                        non_empty_account_types = df['account_type'].notna() & (df['account_type'] != '')
                        non_empty_brands = df['brand'].notna() & (df['brand'] != '')
                        valid_rows = non_empty_account_types.sum() + non_empty_brands.sum()
                        has_valid_data = valid_rows > len(df) * 0.1
                    
                    if has_analysis_fields and has_valid_data:
                        # ç›´æ¥å¤åˆ¶ä¸ºmergedæ–‡ä»¶
                        import shutil
                        merged_path = os.path.join(RESULTS_FOLDER, merged_file)
                        shutil.copy2(file_path, merged_path)
                        custom_logger.info('âœ… CSV already contains analysis results, copied as merged file')
                        merge_success = True
                    else:
                        # æ‰§è¡Œåˆå¹¶é€»è¾‘
                        custom_logger.info('ğŸ”— Merging results with original data...')
                        analysis_tasks[task_id]['progress'] = 'Merging results...'
                        merge_success = merge_results_with_original(file_path, os.path.join(RESULTS_FOLDER, brand_file), merged_file, custom_logger)
                except Exception as e:
                    custom_logger.error(f'âŒ Error checking CSV format: {e}')
                    merge_success = False
            else:
                # æ‰§è¡Œåˆå¹¶é€»è¾‘
                custom_logger.info('ğŸ”— Merging results with original data...')
                analysis_tasks[task_id]['progress'] = 'Merging results...'
                merge_success = merge_results_with_original(file_path, os.path.join(RESULTS_FOLDER, brand_file), merged_file, custom_logger)
            
            # é¦–å…ˆå®šä¹‰å“ç‰Œç›¸å…³è´¦å·ï¼šæœ‰extracted_brand_nameæˆ–è€…è¢«æ ‡è®°ä¸ºå“ç‰Œ/çŸ©é˜µè´¦å·
            brand_related_results = [r for r in results if 
                                   (r.extracted_brand_name and r.extracted_brand_name.strip()) or 
                                   r.is_brand or r.is_matrix_account]
            non_brand_results = [r for r in results if r not in brand_related_results]
            
            # æŒ‰ç…§account_typeç»Ÿè®¡æ€»åˆ›ä½œè€…ä¸­çš„å„ç±»å‹æ•°é‡ï¼ˆäº’æ–¥åˆ†ç±»ï¼‰
            total_creators = len(results)
            official_account_count = sum(1 for r in results if r.is_brand)
            matrix_account_count = sum(1 for r in results if r.is_matrix_account)
            # UGCåˆ›ä½œè€…ï¼šåœ¨å“ç‰Œç›¸å…³ä¸­ä¸”æ ‡è®°ä¸ºUGCçš„åˆ›ä½œè€…
            ugc_creator_count = sum(1 for r in brand_related_results if r.is_ugc_creator)
            # éå“ç‰Œåˆ›ä½œè€…ï¼šä¸åœ¨å“ç‰Œç›¸å…³åˆ—è¡¨ä¸­çš„æ‰€æœ‰åˆ›ä½œè€…
            non_branded_creator_count = len(non_brand_results)
            
            # å“ç‰Œç›¸å…³è´¦å·ä¸­çš„åˆ†ç±»ç»Ÿè®¡ï¼ˆç”¨äºBrand Related Breakdownï¼‰
            brand_in_related = sum(1 for r in brand_related_results if r.is_brand)
            matrix_in_related = sum(1 for r in brand_related_results if r.is_matrix_account)
            ugc_in_related = sum(1 for r in brand_related_results if r.is_ugc_creator)
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
            analysis_tasks[task_id]['status'] = 'completed'
            analysis_tasks[task_id]['progress'] = 'Analysis completed successfully!'
            analysis_tasks[task_id]['results'] = {
                'total_processed': total_creators,
                'brand_related_count': len(brand_related_results),
                'non_brand_count': total_creators - len(brand_related_results),
                'official_account_count': official_account_count,
                'matrix_account_count': matrix_account_count,
                'ugc_creator_count': ugc_creator_count,
                'non_branded_creator_count': non_branded_creator_count,
                # å„ç±»å‹åœ¨æ€»åˆ›ä½œè€…ä¸­çš„ç™¾åˆ†æ¯”
                'official_account_percentage': round((official_account_count / total_creators) * 100) if total_creators > 0 else 0,
                'matrix_account_percentage': round((matrix_account_count / total_creators) * 100) if total_creators > 0 else 0,
                'ugc_creator_percentage': round((ugc_creator_count / total_creators) * 100) if total_creators > 0 else 0,
                'non_branded_creator_percentage': round((non_branded_creator_count / total_creators) * 100) if total_creators > 0 else 0,
                # Brand Related Breakdown - åœ¨å“ç‰Œç›¸å…³è´¦å·ä¸­çš„ç™¾åˆ†æ¯”
                'brand_in_related': brand_in_related,
                'matrix_in_related': matrix_in_related,
                'ugc_in_related': ugc_in_related,
                'brand_in_related_percentage': round((brand_in_related / len(brand_related_results)) * 100) if len(brand_related_results) > 0 else 0,
                'matrix_in_related_percentage': round((matrix_in_related / len(brand_related_results)) * 100) if len(brand_related_results) > 0 else 0,
                'ugc_in_related_percentage': round((ugc_in_related / len(brand_related_results)) * 100) if len(brand_related_results) > 0 else 0,
                'brand_file': brand_file,
                'non_brand_file': non_brand_file,
                'merged_file': merged_file if merge_success else None
            }
            
            custom_logger.info(f'âœ… Analysis completed! Total: {len(results)}, Brand Related: {len(brand_related_results)}, Non-Brand: {len(non_brand_results)}')
            
        else:
            analysis_tasks[task_id]['status'] = 'error'
            analysis_tasks[task_id]['progress'] = 'No results generated'
            custom_logger.error('âŒ No results generated')
            
    except Exception as e:
        print(f"Error in run_csv_analysis for task {task_id}: {str(e)}")
        
        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºé”™è¯¯
        error_msg = f"Analysis failed: {str(e)}"
        
        # ç¡®ä¿ä»»åŠ¡ä»ç„¶å­˜åœ¨
        if task_id in analysis_tasks:
            analysis_tasks[task_id]['status'] = 'error'
            analysis_tasks[task_id]['progress'] = error_msg
            analysis_tasks[task_id]['error_at'] = datetime.now().isoformat()
        
        # ä¹Ÿå°è¯•è®°å½•åˆ°æ—¥å¿—
        try:
            if 'custom_logger' in locals():
                custom_logger.error(f'âŒ Analysis error: {str(e)}')
        except:
            pass
    
    finally:
        # æ€§èƒ½ç»Ÿè®¡
        if 'start_time' in locals():
            execution_time = time.time() - start_time
            print(f"â±ï¸ [CSV Worker] Task {task_id} execution time: {execution_time:.2f}s")
            
            # æ·»åŠ æ€§èƒ½ä¿¡æ¯åˆ°ä»»åŠ¡æ•°æ®ä¸­
            if task_id in analysis_tasks:
                analysis_tasks[task_id]['execution_time'] = execution_time
                analysis_tasks[task_id]['finished_at'] = datetime.now().isoformat()
        
        # æ¸…ç†è‡ªå®šä¹‰logger
        try:
            if 'custom_logger' in locals():
                custom_logger.handlers = []
        except:
            pass
        
        print(f"ğŸ [CSV Worker] CSV analysis thread for task {task_id} finished")

def check_and_process_analyzed_csv(csv_path, logger):
    """æ£€æŸ¥CSVæ˜¯å¦å·²ç»åŒ…å«åˆ†æç»“æœï¼Œå¦‚æœæ˜¯åˆ™ç›´æ¥è¯»å–"""
    try:
        import pandas as pd
        from universal_brand_analyzer import CreatorAnalysis
        
        df = pd.read_csv(csv_path)
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«åˆ†æç»“æœå­—æ®µ
        analysis_fields = ['account_type', 'brand', 'email', 'recent_20_posts_views_avg', 
                          'recent_20_posts_like_avg', 'recent_20_posts_share_avg', 
                          'Posting Frequency', 'Stability Score']
        
        has_analysis_fields = all(field in df.columns for field in analysis_fields)
        
        # æ£€æŸ¥å­—æ®µæ˜¯å¦æœ‰å®é™…æ•°æ®ï¼ˆä¸åªæ˜¯ç©ºå€¼ï¼‰
        has_valid_data = False
        if has_analysis_fields:
            # æ£€æŸ¥æ˜¯å¦æœ‰éç©ºçš„åˆ†æç»“æœ
            non_empty_account_types = df['account_type'].notna() & (df['account_type'] != '')
            non_empty_brands = df['brand'].notna() & (df['brand'] != '')
            
            # å¦‚æœæœ‰è‡³å°‘10%çš„è¡Œæœ‰æœ‰æ•ˆçš„account_typeæˆ–brandæ•°æ®ï¼Œåˆ™è®¤ä¸ºå·²ç»åˆ†æè¿‡
            valid_rows = non_empty_account_types.sum() + non_empty_brands.sum()
            has_valid_data = valid_rows > len(df) * 0.1
        
        if has_analysis_fields and has_valid_data:
            logger.info('âœ… CSV already contains analysis results, reading existing data...')
            
            results = []
            for _, row in df.iterrows():
                # è½¬æ¢è´¦æˆ·ç±»å‹å›å¸ƒå°”å€¼
                account_type = str(row.get('account_type', 'ugc creator')).lower()
                is_brand = account_type == 'official account'
                is_matrix_account = account_type == 'matrix account'
                is_ugc_creator = account_type in ['ugc creator', 'non-branded creator']
                
                # åˆ›å»ºCreatorAnalysiså¯¹è±¡
                analysis = CreatorAnalysis(
                    video_id=str(row.get('video_id', '')),
                    author_unique_id=str(row.get('user_unique_id', '')),
                    author_link=f"https://www.tiktok.com/@{row.get('user_unique_id', '')}",
                    signature=str(row.get('title', '')),  # ä½¿ç”¨titleä½œä¸ºsignature
                    is_brand=is_brand,
                    is_matrix_account=is_matrix_account,
                    is_ugc_creator=is_ugc_creator,
                    extracted_brand_name=str(row.get('brand', '')),
                    brand_confidence=float(row.get('brand_confidence', 0)) if pd.notna(row.get('brand_confidence')) else 0.0,
                    analysis_details=str(row.get('analysis_details', '')),
                    author_followers_count=int(row.get('follower_count', 0)) if pd.notna(row.get('follower_count')) else 0,
                    author_followings_count=0,  # ä¸åœ¨åŸå§‹CSVä¸­
                    videoCount=0,  # ä¸åœ¨åŸå§‹CSVä¸­
                    author_avatar='',
                    create_times=str(row.get('date', '')),
                    email=str(row.get('email', '')),
                    recent_20_posts_views_avg=float(row.get('recent_20_posts_views_avg', 0)) if pd.notna(row.get('recent_20_posts_views_avg')) else 0.0,
                    recent_20_posts_like_avg=float(row.get('recent_20_posts_like_avg', 0)) if pd.notna(row.get('recent_20_posts_like_avg')) else 0.0,
                    recent_20_posts_share_avg=float(row.get('recent_20_posts_share_avg', 0)) if pd.notna(row.get('recent_20_posts_share_avg')) else 0.0,
                    posting_frequency=float(row.get('Posting Frequency', 0)) if pd.notna(row.get('Posting Frequency')) else 0.0,
                    stability_score=float(row.get('Stability Score', 0)) if pd.notna(row.get('Stability Score')) else 0.0
                )
                results.append(analysis)
            
            logger.info(f'ğŸ“Š Read {len(results)} existing analysis results from CSV')
            return results
        else:
            if has_analysis_fields and not has_valid_data:
                logger.info('ğŸ“‹ CSV has analysis fields but no valid data, will perform new analysis')
            else:
                logger.info('âŒ CSV does not contain analysis fields, will perform new analysis')
            return None
            
    except Exception as e:
        logger.error(f'âŒ Error checking CSV: {e}')
        return None

def merge_results_with_original(original_csv_path, analysis_csv_path, output_filename, logger):
    """åˆå¹¶åˆ†æç»“æœä¸åŸå§‹æ•°æ®"""
    try:
        import pandas as pd
        
        logger.info(f'ğŸ“Š Loading original data from: {original_csv_path}')
        
        # è¯»å–åŸå§‹CSVæ–‡ä»¶
        original_df = pd.read_csv(original_csv_path)
        logger.info(f'ğŸ“„ Original file contains {len(original_df)} rows')
        
        # æ£€æŸ¥åˆ†æç»“æœæ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(analysis_csv_path):
            logger.error(f'âŒ Analysis file not found: {analysis_csv_path}')
            return False
        
        # è¯»å–åˆ†ææ•°æ®æ–‡ä»¶
        analysis_df = pd.read_csv(analysis_csv_path)
        logger.info(f'ğŸ“Š Analysis file contains {len(analysis_df)} rows')
        
        # æ£€æŸ¥å¿…éœ€çš„åˆ—
        if 'video_id' not in original_df.columns:
            logger.error('âŒ Original file missing video_id column')
            return False
        
        if 'video_id' not in analysis_df.columns:
            logger.error('âŒ Analysis file missing video_id column')
            return False
        
        # å‡†å¤‡è¦åˆå¹¶çš„å­—æ®µæ˜ å°„
        field_mapping = {
            'account_type': 'account_type',
            'brand': 'brand',
            'email': 'email', 
            'recent_20_posts_views_avg': 'recent_20_posts_views_avg',
            'recent_20_posts_like_avg': 'recent_20_posts_like_avg', 
            'recent_20_posts_share_avg': 'recent_20_posts_share_avg',
            'posting_frequency': 'Posting Frequency',
            'stability_score': 'Stability Score',
            'brand_confidence': 'brand_confidence',
            'analysis_details': 'analysis_details'
        }
        
        # æ£€æŸ¥åŸå§‹æ–‡ä»¶æ˜¯å¦æœ‰è¿™äº›å­—æ®µï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆ›å»º
        for analysis_field, original_field in field_mapping.items():
            if original_field not in original_df.columns:
                logger.info(f'â• Creating missing column: {original_field}')
                original_df[original_field] = ''
        
        # æ‰§è¡Œåˆå¹¶
        logger.info('ğŸ”„ Merging data...')
        matched_count = 0
        updated_count = 0
        
        for index, row in original_df.iterrows():
            video_id = row['video_id']
            
            # æŸ¥æ‰¾åŒ¹é…çš„åˆ†ææ•°æ®
            match = analysis_df[analysis_df['video_id'] == video_id]
            
            if not match.empty:
                matched_count += 1
                match_row = match.iloc[0]
                
                # æ›´æ–°æ‰€æœ‰å­—æ®µï¼ˆå¼ºåˆ¶æ›´æ–°ï¼‰
                for analysis_field, original_field in field_mapping.items():
                    if analysis_field in analysis_df.columns and original_field in original_df.columns:
                        original_df.at[index, original_field] = match_row[analysis_field]
                        updated_count += 1
        
        logger.info(f'âœ… Matched {matched_count} video IDs, updated {updated_count} field values')
        
        # ä¿å­˜åˆå¹¶åçš„æ–‡ä»¶
        output_path = os.path.join(RESULTS_FOLDER, output_filename)
        logger.info(f'ğŸ’¾ Saving merged file to: {output_path}')
        original_df.to_csv(output_path, index=False)
        
        logger.info('ğŸ‰ Merge completed successfully!')
        return True
        
    except Exception as e:
        logger.error(f'âŒ Merge failed: {str(e)}')
        return False

@app.route('/api/health', methods=['GET'])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    return jsonify({'status': 'healthy', 'message': 'Brand Analyzer API is running'})

@app.route('/api/thread-pool-status', methods=['GET'])
def get_thread_pool_status():
    """è·å–çº¿ç¨‹æ± çŠ¶æ€ä¿¡æ¯ - ç›‘æ§åˆ†ææ€§èƒ½"""
    # ç»Ÿè®¡æ­£åœ¨è¿è¡Œçš„ä»»åŠ¡
    running_tasks = sum(1 for task in analysis_tasks.values() if task.get('status') == 'running')
    pending_tasks = sum(1 for task in analysis_tasks.values() if task.get('status') == 'pending')
    completed_tasks = sum(1 for task in analysis_tasks.values() if task.get('status') in ['completed', 'error'])
    
    # æ£€æŸ¥æœ‰å¤šå°‘Futureå¯¹è±¡ä»åœ¨è¿è¡Œ
    active_futures = 0
    for task in analysis_tasks.values():
        future = task.get('future')
        if future and not future.done():
            active_futures += 1
    
    return jsonify({
        'thread_pool': {
            'max_workers': MAX_WORKERS,
            'active_futures': active_futures,
            'available_workers': MAX_WORKERS - active_futures
        },
        'tasks': {
            'total': len(analysis_tasks),
            'running': running_tasks,
            'pending': pending_tasks,
            'completed': completed_tasks
        },
        'performance': {
            'status': 'optimal' if active_futures < MAX_WORKERS else 'busy',
            'efficiency': f"{((MAX_WORKERS - active_futures) / MAX_WORKERS * 100):.1f}% available"
        }
    })

@app.route('/api/upload', methods=['POST'])
def upload_file():
    if 'file' not in request.files:
        return jsonify({'error': 'No file selected'}), 400
    
    file = request.files['file']
    if file.filename == '':
        return jsonify({'error': 'No file selected'}), 400
    
    if file and allowed_file(file.filename):
        # ç”Ÿæˆå”¯ä¸€çš„ä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # ä¿å­˜ä¸Šä¼ çš„æ–‡ä»¶
        filename = secure_filename(file.filename)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{timestamp}_{filename}"
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)
        
        # æ£€æµ‹æ–‡ä»¶ç±»å‹
        is_csv = file.filename.lower().endswith('.csv')
        file_type = 'CSV' if is_csv else 'JSON'
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        analysis_tasks[task_id] = {
            'status': 'pending',
            'progress': f'{file_type} file uploaded successfully, preparing to start analysis...',
            'file_path': file_path,
            'filename': file.filename,
            'file_type': file_type.lower(),
            'logs': [],
            'created_at': datetime.now().isoformat()
        }
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡Œåˆ†æä»»åŠ¡ - é«˜æ•ˆå¹¶å‘å¤„ç†
        future = thread_pool.submit(run_analysis, task_id, file_path, is_csv)
        # å­˜å‚¨Futureå¯¹è±¡ä»¥ä¾¿åç»­è·Ÿè¸ª
        analysis_tasks[task_id]['future'] = future
        
        return jsonify({'task_id': task_id})
    
    return jsonify({'error': 'Unsupported file format, please upload JSON or CSV file'}), 400

@app.route('/api/status')
def get_status():
    task_id = request.args.get('task_id')
    if not task_id:
        return jsonify({'error': 'Missing task_id parameter'}), 400
    
    print(f"Getting status for task {task_id}")
    print(f"Current tasks in memory: {list(analysis_tasks.keys())}")
    
    if task_id in analysis_tasks:
        task_data = analysis_tasks[task_id]
        print(f"Task {task_id} status: {task_data.get('status', 'unknown')}")
        
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        debug_info = {
            'task_exists': True,
            'current_tasks_count': len(analysis_tasks),
            'task_created_at': task_data.get('created_at', 'unknown')
        }
        
        # åˆ›å»ºå¯JSONåºåˆ—åŒ–çš„å“åº”æ•°æ®ï¼ˆæ’é™¤Futureå¯¹è±¡ï¼‰
        response_data = {k: v for k, v in task_data.items() if k != 'future'}
        response_data['debug'] = debug_info
        
        # æ·»åŠ çº¿ç¨‹æ± çŠ¶æ€ä¿¡æ¯
        future = task_data.get('future')
        if future:
            response_data['thread_status'] = {
                'is_running': not future.done(),
                'is_completed': future.done()
            }
        
        return jsonify(response_data)
    
    print(f"Task {task_id} not found in analysis_tasks")
    return jsonify({
        'error': 'Task not found', 
        'debug': {
            'task_exists': False,
            'current_tasks_count': len(analysis_tasks),
            'available_tasks': list(analysis_tasks.keys())[:5]  # åªæ˜¾ç¤ºå‰5ä¸ªä»»åŠ¡ID
        }
    }), 404

@app.route('/api/analyze-csv', methods=['POST'])
def analyze_csv():
    """ç›´æ¥åˆ†ææŒ‡å®šçš„CSVæ–‡ä»¶"""
    try:
        data = request.get_json()
        if not data or 'file_path' not in data:
            return jsonify({'error': 'Missing file_path parameter'}), 400
        
        file_path = data['file_path']
        
        # ç¡®ä¿æ–‡ä»¶å­˜åœ¨
        full_path = os.path.join(os.getcwd(), file_path)
        if not os.path.exists(full_path):
            return jsonify({'error': f'File not found: {file_path}'}), 404
        
        # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦ä¸ºCSV
        if not file_path.lower().endswith('.csv'):
            return jsonify({'error': 'Only CSV files are supported'}), 400
        
        # ç”Ÿæˆå”¯ä¸€çš„ä»»åŠ¡ID
        task_id = str(uuid.uuid4())
        
        # åˆå§‹åŒ–ä»»åŠ¡çŠ¶æ€
        analysis_tasks[task_id] = {
            'status': 'pending',
            'progress': 'CSV file detected, starting analysis...',
            'file_path': full_path,
            'filename': os.path.basename(file_path),
            'file_type': 'csv',
            'logs': [],
            'created_at': datetime.now().isoformat()
        }
        
        # åœ¨çº¿ç¨‹æ± ä¸­è¿è¡ŒCSVåˆ†æä»»åŠ¡ - é«˜æ•ˆå¹¶å‘å¤„ç†
        future = thread_pool.submit(run_csv_analysis, task_id, full_path)
        # å­˜å‚¨Futureå¯¹è±¡ä»¥ä¾¿åç»­è·Ÿè¸ª
        analysis_tasks[task_id]['future'] = future
        
        return jsonify({'task_id': task_id})
        
    except Exception as e:
        return jsonify({'error': f'Analysis failed: {str(e)}'}), 500

@app.route('/api/download')
def download_file():
    task_id = request.args.get('task_id')
    file_type = request.args.get('file_type')
    
    if not task_id or not file_type:
        return jsonify({'error': 'Missing required parameters'}), 400
    
    if task_id not in analysis_tasks:
        return jsonify({'error': 'Task not found'}), 404
    
    task = analysis_tasks[task_id]
    if task['status'] != 'completed':
        return jsonify({'error': 'Analysis not completed yet'}), 400
    
    if file_type == 'brand_related':
        filename = task['results']['brand_file']
    elif file_type == 'non_brand':
        filename = task['results']['non_brand_file']
    elif file_type == 'merged':
        filename = task['results'].get('merged_file')
        if not filename:
            return jsonify({'error': 'Merged file not available'}), 400
    else:
        return jsonify({'error': 'Invalid file type'}), 400
    
    file_path = os.path.join(RESULTS_FOLDER, filename)
    if os.path.exists(file_path):
        return send_file(file_path, as_attachment=True, download_name=filename)
    
    return jsonify({'error': 'File not found'}), 404

@app.route('/api/logs')
def get_logs():
    task_id = request.args.get('task_id')
    if not task_id:
        return jsonify({'error': 'Missing task_id parameter'}), 400
    
    print(f"Getting logs for task {task_id}")
    
    if task_id in analysis_tasks:
        logs = analysis_tasks[task_id].get('logs', [])
        print(f"Task {task_id} has {len(logs)} log entries")
        return jsonify({'logs': logs})
    
    print(f"Task {task_id} not found for logs")
    return jsonify({'error': 'Task not found'}), 404

if __name__ == '__main__':
    start_cleanup_thread() # å¯åŠ¨æ¸…ç†çº¿ç¨‹
    app.run(debug=True, host='0.0.0.0', port=5000) 